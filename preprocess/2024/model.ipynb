{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 模型训练/调参/预测"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5b23bf8db9c940a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:27:27.859624Z",
     "start_time": "2024-10-30T00:27:26.864018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "\n",
    "from constant import *\n",
    "from sklearn.metrics import roc_curve\n",
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
    "import lightgbm as lgb\n",
    "from data import loader,exporter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:27:27.875655Z",
     "start_time": "2024-10-30T00:27:27.861629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ks_stat(y_true, y_pred):\n",
    "    \"\"\"计算KS值的自定义评估函数\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)  # 计算ROC曲线\n",
    "    ks_value = np.max(np.abs(tpr - fpr))  # KS统计量\n",
    "    return ks_value\n",
    "\n",
    "\n",
    "def lgb_ks_eval(y_pred, dataset):\n",
    "    \"\"\"用于LightGBM的自定义KS评估函数\"\"\"\n",
    "    y_true = dataset.get_label()\n",
    "    ks_value = ks_stat(y_true, y_pred)\n",
    "    # 返回 (名称, 计算的 KS 值, 是否越高越好)\n",
    "    return 'ks', ks_value, True\n",
    "\n",
    "\n",
    "# 自定义目标函数，用于贝叶斯优化\n",
    "def objective(params, X, y, n_folds=5):\n",
    "    \"\"\"贝叶斯优化的目标函数，返回负的整体验证集KS分数\"\"\"\n",
    "    # 设置模型参数\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'binary'\n",
    "    params['verbose'] = -1\n",
    "    params['early_stopping_round'] = 20\n",
    "    params['n_estimators'] = 10000\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2023)\n",
    "\n",
    "    # 存储每一折的预测结果\n",
    "    final_predictions = np.zeros(len(X))\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        # 建立LightGBM训练集\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "        # 训练LightGBM模型\n",
    "        model = lgb.train(params,\n",
    "                          lgb_train,\n",
    "                          valid_sets=[lgb_train, lgb_valid],\n",
    "                          feval=lgb_ks_eval,\n",
    "                          callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                                     lgb.log_evaluation(10)])\n",
    "\n",
    "        # 验证集预测概率\n",
    "        y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "        # 将每一折的预测结果填入到相应的索引位置\n",
    "        final_predictions[valid_idx] = y_pred\n",
    "\n",
    "    # 计算整体的KS分数\n",
    "    overall_score = ks_stat(y, final_predictions)\n",
    "\n",
    "    # 返回负的KS分数作为最小化目标\n",
    "    return {'loss': -overall_score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# 超参数空间\n",
    "param_space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),  # 学习率\n",
    "    'num_leaves': hp.choice('num_leaves', np.arange(20, 150, dtype=int)),  # 叶子数\n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 12, dtype=int)),  # 树的最大深度\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.001, 10),  # 子叶节点的最小权重\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # 样本列采样率\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 20),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # 样本采样率\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),  # L1正则化\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)  # L2正则化\n",
    "}\n",
    "\n",
    "\n",
    "# 贝叶斯优化调参函数\n",
    "def bayesian_optimize_lgbm(X, y, param_space, max_evals=10):\n",
    "    trials = Trials()\n",
    "\n",
    "    # 使用fmin函数进行贝叶斯优化\n",
    "    best_params = fmin(\n",
    "        fn=lambda params: objective(params, X, y),  # 优化目标\n",
    "        space=param_space,  # 参数空间\n",
    "        algo=tpe.suggest,  # 使用TPE算法\n",
    "        max_evals=max_evals,  # 最大评估次数\n",
    "        trials=trials  # 记录每次评估的结果\n",
    "    )\n",
    "\n",
    "    return best_params, trials"
   ],
   "id": "fb0bbd61242dac48",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:29:51.908315Z",
     "start_time": "2024-10-30T00:27:27.877621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "# df_target = pd.read_csv(f'{dir_preprocess}/target.csv')\n",
    "# df_flat = pd.read_csv(f'{dir_preprocess}/expend_1.csv')\n",
    "# df_v1 = pd.read_csv(f'{dir_preprocess}/v1.csv')\n",
    "# df = df_target.merge(df_flat, left_on=['CUST_NO'], right_on=['CUST_NO'], how='left').merge(df_v1, left_on=['CUST_NO','SRC','FLAG'], right_on=['CUST_NO','SRC','FLAG'], how='left')\n",
    "\n",
    "X = loader.to_df(os.path.join(dir_preprocess,'v2.csv'))\n",
    "y = X.pop(\"FLAG\")  # 目标标签列\n",
    "\n",
    "# 执行贝叶斯优化\n",
    "best_params, trials = bayesian_optimize_lgbm(X, y, param_space, max_evals=1)\n",
    "\n",
    "print(\"Best parameters found:\", best_params)"
   ],
   "id": "467c020715be8573",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.332345\ttraining's ks: 0.910028\tvalid_1's binary_logloss: 0.341138\tvalid_1's ks: 0.898358\n",
      "[20]\ttraining's binary_logloss: 0.22029\ttraining's ks: 0.938615\tvalid_1's binary_logloss: 0.232967\tvalid_1's ks: 0.928791\n",
      "[30]\ttraining's binary_logloss: 0.171161\ttraining's ks: 0.950853\tvalid_1's binary_logloss: 0.186766\tvalid_1's ks: 0.938443\n",
      "[40]\ttraining's binary_logloss: 0.142482\ttraining's ks: 0.956221\tvalid_1's binary_logloss: 0.160914\tvalid_1's ks: 0.943564\n",
      "[50]\ttraining's binary_logloss: 0.127313\ttraining's ks: 0.959372\tvalid_1's binary_logloss: 0.148059\tvalid_1's ks: 0.945239\n",
      "[60]\ttraining's binary_logloss: 0.113568\ttraining's ks: 0.963829\tvalid_1's binary_logloss: 0.137219\tvalid_1's ks: 0.947602\n",
      "[70]\ttraining's binary_logloss: 0.102978\ttraining's ks: 0.966932\tvalid_1's binary_logloss: 0.129256\tvalid_1's ks: 0.948981\n",
      "[80]\ttraining's binary_logloss: 0.0943909\ttraining's ks: 0.970699\tvalid_1's binary_logloss: 0.122654\tvalid_1's ks: 0.949474\n",
      "[90]\ttraining's binary_logloss: 0.0850778\ttraining's ks: 0.975599\tvalid_1's binary_logloss: 0.116325\tvalid_1's ks: 0.949868\n",
      "[100]\ttraining's binary_logloss: 0.0789615\ttraining's ks: 0.978086\tvalid_1's binary_logloss: 0.112192\tvalid_1's ks: 0.950655\n",
      "[110]\ttraining's binary_logloss: 0.0731272\ttraining's ks: 0.981016\tvalid_1's binary_logloss: 0.108041\tvalid_1's ks: 0.951443\n",
      "[120]\ttraining's binary_logloss: 0.068498\ttraining's ks: 0.98333\tvalid_1's binary_logloss: 0.104617\tvalid_1's ks: 0.95164\n",
      "[130]\ttraining's binary_logloss: 0.0635328\ttraining's ks: 0.986285\tvalid_1's binary_logloss: 0.10149\tvalid_1's ks: 0.951936\n",
      "[140]\ttraining's binary_logloss: 0.0579688\ttraining's ks: 0.988723\tvalid_1's binary_logloss: 0.098142\tvalid_1's ks: 0.952231\n",
      "[150]\ttraining's binary_logloss: 0.0530843\ttraining's ks: 0.99089\tvalid_1's binary_logloss: 0.0946689\tvalid_1's ks: 0.952625\n",
      "[160]\ttraining's binary_logloss: 0.0491836\ttraining's ks: 0.99313\tvalid_1's binary_logloss: 0.0922645\tvalid_1's ks: 0.952921\n",
      "[170]\ttraining's binary_logloss: 0.0453503\ttraining's ks: 0.994509\tvalid_1's binary_logloss: 0.0899608\tvalid_1's ks: 0.953019\n",
      "[180]\ttraining's binary_logloss: 0.042057\ttraining's ks: 0.995543\tvalid_1's binary_logloss: 0.0880807\tvalid_1's ks: 0.953019\n",
      "[190]\ttraining's binary_logloss: 0.0387134\ttraining's ks: 0.996454\tvalid_1's binary_logloss: 0.0862867\tvalid_1's ks: 0.953118\n",
      "[200]\ttraining's binary_logloss: 0.0359085\ttraining's ks: 0.997218\tvalid_1's binary_logloss: 0.0849593\tvalid_1's ks: 0.953315\n",
      "[210]\ttraining's binary_logloss: 0.0329725\ttraining's ks: 0.997932\tvalid_1's binary_logloss: 0.0834178\tvalid_1's ks: 0.953216\n",
      "  0%|          | 0/1 [00:28<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.337443\ttraining's ks: 0.906457\tvalid_1's binary_logloss: 0.346554\tvalid_1's ks: 0.897575\n",
      "[20]\ttraining's binary_logloss: 0.225204\ttraining's ks: 0.937827\tvalid_1's binary_logloss: 0.23841\tvalid_1's ks: 0.929977\n",
      "[30]\ttraining's binary_logloss: 0.173382\ttraining's ks: 0.949005\tvalid_1's binary_logloss: 0.189829\tvalid_1's ks: 0.940516\n",
      "[40]\ttraining's binary_logloss: 0.143597\ttraining's ks: 0.954743\tvalid_1's binary_logloss: 0.163282\tvalid_1's ks: 0.94544\n",
      "[50]\ttraining's binary_logloss: 0.128113\ttraining's ks: 0.958559\tvalid_1's binary_logloss: 0.150372\tvalid_1's ks: 0.946818\n",
      "[60]\ttraining's binary_logloss: 0.113332\ttraining's ks: 0.962671\tvalid_1's binary_logloss: 0.138709\tvalid_1's ks: 0.948887\n",
      "[70]\ttraining's binary_logloss: 0.103297\ttraining's ks: 0.966365\tvalid_1's binary_logloss: 0.130915\tvalid_1's ks: 0.949478\n",
      "[80]\ttraining's binary_logloss: 0.0955634\ttraining's ks: 0.969566\tvalid_1's binary_logloss: 0.125345\tvalid_1's ks: 0.950167\n",
      "[90]\ttraining's binary_logloss: 0.0873074\ttraining's ks: 0.974072\tvalid_1's binary_logloss: 0.119529\tvalid_1's ks: 0.950462\n",
      "[100]\ttraining's binary_logloss: 0.0809315\ttraining's ks: 0.97678\tvalid_1's binary_logloss: 0.114948\tvalid_1's ks: 0.95125\n",
      "[110]\ttraining's binary_logloss: 0.0734524\ttraining's ks: 0.98109\tvalid_1's binary_logloss: 0.109704\tvalid_1's ks: 0.951447\n",
      "[120]\ttraining's binary_logloss: 0.0679151\ttraining's ks: 0.983576\tvalid_1's binary_logloss: 0.105983\tvalid_1's ks: 0.952235\n",
      "[130]\ttraining's binary_logloss: 0.0619562\ttraining's ks: 0.986654\tvalid_1's binary_logloss: 0.102053\tvalid_1's ks: 0.952137\n",
      "[140]\ttraining's binary_logloss: 0.0567365\ttraining's ks: 0.989215\tvalid_1's binary_logloss: 0.098878\tvalid_1's ks: 0.952432\n",
      "[150]\ttraining's binary_logloss: 0.0512223\ttraining's ks: 0.991579\tvalid_1's binary_logloss: 0.0953436\tvalid_1's ks: 0.95322\n",
      "[160]\ttraining's binary_logloss: 0.0477507\ttraining's ks: 0.993007\tvalid_1's binary_logloss: 0.0931193\tvalid_1's ks: 0.953319\n",
      "[170]\ttraining's binary_logloss: 0.0438318\ttraining's ks: 0.99478\tvalid_1's binary_logloss: 0.0909588\tvalid_1's ks: 0.95391\n",
      "[180]\ttraining's binary_logloss: 0.0398828\ttraining's ks: 0.996159\tvalid_1's binary_logloss: 0.0885996\tvalid_1's ks: 0.953614\n",
      "[190]\ttraining's binary_logloss: 0.0363542\ttraining's ks: 0.997291\tvalid_1's binary_logloss: 0.0867797\tvalid_1's ks: 0.952826\n",
      "  0%|          | 0/1 [00:54<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.338616\ttraining's ks: 0.907025\tvalid_1's binary_logloss: 0.344045\tvalid_1's ks: 0.898749\n",
      "[20]\ttraining's binary_logloss: 0.22559\ttraining's ks: 0.938616\tvalid_1's binary_logloss: 0.235317\tvalid_1's ks: 0.932729\n",
      "[30]\ttraining's binary_logloss: 0.171961\ttraining's ks: 0.94945\tvalid_1's binary_logloss: 0.186429\tvalid_1's ks: 0.942973\n",
      "[40]\ttraining's binary_logloss: 0.143303\ttraining's ks: 0.954941\tvalid_1's binary_logloss: 0.161252\tvalid_1's ks: 0.946617\n",
      "[50]\ttraining's binary_logloss: 0.125248\ttraining's ks: 0.959102\tvalid_1's binary_logloss: 0.146443\tvalid_1's ks: 0.94839\n",
      "[60]\ttraining's binary_logloss: 0.111361\ttraining's ks: 0.963263\tvalid_1's binary_logloss: 0.136091\tvalid_1's ks: 0.950064\n",
      "[70]\ttraining's binary_logloss: 0.101392\ttraining's ks: 0.967449\tvalid_1's binary_logloss: 0.128192\tvalid_1's ks: 0.95095\n",
      "[80]\ttraining's binary_logloss: 0.0927891\ttraining's ks: 0.971167\tvalid_1's binary_logloss: 0.122476\tvalid_1's ks: 0.952132\n",
      "[90]\ttraining's binary_logloss: 0.0848697\ttraining's ks: 0.974639\tvalid_1's binary_logloss: 0.116397\tvalid_1's ks: 0.952723\n",
      "[100]\ttraining's binary_logloss: 0.0779534\ttraining's ks: 0.977963\tvalid_1's binary_logloss: 0.111544\tvalid_1's ks: 0.953511\n",
      "[110]\ttraining's binary_logloss: 0.0715153\ttraining's ks: 0.981779\tvalid_1's binary_logloss: 0.107009\tvalid_1's ks: 0.953708\n",
      "[120]\ttraining's binary_logloss: 0.0654234\ttraining's ks: 0.984857\tvalid_1's binary_logloss: 0.103374\tvalid_1's ks: 0.953905\n",
      "[130]\ttraining's binary_logloss: 0.0603293\ttraining's ks: 0.987245\tvalid_1's binary_logloss: 0.100027\tvalid_1's ks: 0.954201\n",
      "[140]\ttraining's binary_logloss: 0.05578\ttraining's ks: 0.989314\tvalid_1's binary_logloss: 0.0970309\tvalid_1's ks: 0.954299\n",
      "  0%|          | 0/1 [01:16<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.337104\ttraining's ks: 0.90865\tvalid_1's binary_logloss: 0.341105\tvalid_1's ks: 0.902295\n",
      "[20]\ttraining's binary_logloss: 0.22538\ttraining's ks: 0.938542\tvalid_1's binary_logloss: 0.233273\tvalid_1's ks: 0.934699\n",
      "[30]\ttraining's binary_logloss: 0.173772\ttraining's ks: 0.949918\tvalid_1's binary_logloss: 0.185579\tvalid_1's ks: 0.94445\n",
      "[40]\ttraining's binary_logloss: 0.144271\ttraining's ks: 0.955556\tvalid_1's binary_logloss: 0.159764\tvalid_1's ks: 0.948488\n",
      "[50]\ttraining's binary_logloss: 0.127172\ttraining's ks: 0.959299\tvalid_1's binary_logloss: 0.14567\tvalid_1's ks: 0.950261\n",
      "[60]\ttraining's binary_logloss: 0.116241\ttraining's ks: 0.961564\tvalid_1's binary_logloss: 0.136783\tvalid_1's ks: 0.951738\n",
      "[70]\ttraining's binary_logloss: 0.104326\ttraining's ks: 0.965578\tvalid_1's binary_logloss: 0.127095\tvalid_1's ks: 0.952822\n",
      "[80]\ttraining's binary_logloss: 0.095453\ttraining's ks: 0.96996\tvalid_1's binary_logloss: 0.120635\tvalid_1's ks: 0.953905\n",
      "[90]\ttraining's binary_logloss: 0.0883994\ttraining's ks: 0.972964\tvalid_1's binary_logloss: 0.115809\tvalid_1's ks: 0.954004\n",
      "[100]\ttraining's binary_logloss: 0.0808164\ttraining's ks: 0.976953\tvalid_1's binary_logloss: 0.110732\tvalid_1's ks: 0.954693\n",
      "[110]\ttraining's binary_logloss: 0.0737744\ttraining's ks: 0.980425\tvalid_1's binary_logloss: 0.10598\tvalid_1's ks: 0.955383\n",
      "[120]\ttraining's binary_logloss: 0.0680743\ttraining's ks: 0.983626\tvalid_1's binary_logloss: 0.10227\tvalid_1's ks: 0.955186\n",
      "[130]\ttraining's binary_logloss: 0.0642049\ttraining's ks: 0.985522\tvalid_1's binary_logloss: 0.0999947\tvalid_1's ks: 0.955481\n",
      "[140]\ttraining's binary_logloss: 0.059748\ttraining's ks: 0.987763\tvalid_1's binary_logloss: 0.0972607\tvalid_1's ks: 0.95558\n",
      "[150]\ttraining's binary_logloss: 0.0548411\ttraining's ks: 0.9902\tvalid_1's binary_logloss: 0.0938347\tvalid_1's ks: 0.955777\n",
      "[160]\ttraining's binary_logloss: 0.0498851\ttraining's ks: 0.992638\tvalid_1's binary_logloss: 0.0909003\tvalid_1's ks: 0.956269\n",
      "[170]\ttraining's binary_logloss: 0.0457985\ttraining's ks: 0.994115\tvalid_1's binary_logloss: 0.0884511\tvalid_1's ks: 0.956368\n",
      "[180]\ttraining's binary_logloss: 0.041398\ttraining's ks: 0.995568\tvalid_1's binary_logloss: 0.0857593\tvalid_1's ks: 0.956762\n",
      "[190]\ttraining's binary_logloss: 0.0387756\ttraining's ks: 0.996405\tvalid_1's binary_logloss: 0.0841746\tvalid_1's ks: 0.956663\n",
      "[200]\ttraining's binary_logloss: 0.0354704\ttraining's ks: 0.997538\tvalid_1's binary_logloss: 0.0820478\tvalid_1's ks: 0.957057\n",
      "[210]\ttraining's binary_logloss: 0.0327702\ttraining's ks: 0.998276\tvalid_1's binary_logloss: 0.0805149\tvalid_1's ks: 0.956959\n",
      "[220]\ttraining's binary_logloss: 0.0305356\ttraining's ks: 0.998572\tvalid_1's binary_logloss: 0.0793\tvalid_1's ks: 0.957254\n",
      "  0%|          | 0/1 [01:45<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.337161\ttraining's ks: 0.903602\tvalid_1's binary_logloss: 0.344037\tvalid_1's ks: 0.896385\n",
      "[20]\ttraining's binary_logloss: 0.225984\ttraining's ks: 0.937089\tvalid_1's binary_logloss: 0.235649\tvalid_1's ks: 0.931646\n",
      "[30]\ttraining's binary_logloss: 0.175532\ttraining's ks: 0.94812\tvalid_1's binary_logloss: 0.190038\tvalid_1's ks: 0.943957\n",
      "[40]\ttraining's binary_logloss: 0.146352\ttraining's ks: 0.953168\tvalid_1's binary_logloss: 0.164539\tvalid_1's ks: 0.948094\n",
      "[50]\ttraining's binary_logloss: 0.128251\ttraining's ks: 0.957772\tvalid_1's binary_logloss: 0.149308\tvalid_1's ks: 0.951246\n",
      "[60]\ttraining's binary_logloss: 0.116569\ttraining's ks: 0.960555\tvalid_1's binary_logloss: 0.140293\tvalid_1's ks: 0.952428\n",
      "[70]\ttraining's binary_logloss: 0.105514\ttraining's ks: 0.963977\tvalid_1's binary_logloss: 0.132399\tvalid_1's ks: 0.953511\n",
      "[80]\ttraining's binary_logloss: 0.0983528\ttraining's ks: 0.967104\tvalid_1's binary_logloss: 0.126758\tvalid_1's ks: 0.953905\n",
      "[90]\ttraining's binary_logloss: 0.0906172\ttraining's ks: 0.970453\tvalid_1's binary_logloss: 0.12117\tvalid_1's ks: 0.954398\n",
      "[100]\ttraining's binary_logloss: 0.0841763\ttraining's ks: 0.973605\tvalid_1's binary_logloss: 0.116484\tvalid_1's ks: 0.954989\n",
      "[110]\ttraining's binary_logloss: 0.0759372\ttraining's ks: 0.977766\tvalid_1's binary_logloss: 0.110681\tvalid_1's ks: 0.955087\n",
      "[120]\ttraining's binary_logloss: 0.0711596\ttraining's ks: 0.980179\tvalid_1's binary_logloss: 0.107426\tvalid_1's ks: 0.955777\n",
      "[130]\ttraining's binary_logloss: 0.0656789\ttraining's ks: 0.983404\tvalid_1's binary_logloss: 0.10362\tvalid_1's ks: 0.955777\n",
      "[140]\ttraining's binary_logloss: 0.0608308\ttraining's ks: 0.985965\tvalid_1's binary_logloss: 0.100533\tvalid_1's ks: 0.956368\n",
      "[150]\ttraining's binary_logloss: 0.0559994\ttraining's ks: 0.988624\tvalid_1's binary_logloss: 0.0971458\tvalid_1's ks: 0.957057\n",
      "[160]\ttraining's binary_logloss: 0.0517994\ttraining's ks: 0.990594\tvalid_1's binary_logloss: 0.094244\tvalid_1's ks: 0.956959\n",
      "[170]\ttraining's binary_logloss: 0.0462828\ttraining's ks: 0.99345\tvalid_1's binary_logloss: 0.090752\tvalid_1's ks: 0.957254\n",
      "[180]\ttraining's binary_logloss: 0.0422179\ttraining's ks: 0.9951\tvalid_1's binary_logloss: 0.0880727\tvalid_1's ks: 0.957746\n",
      "[190]\ttraining's binary_logloss: 0.0388614\ttraining's ks: 0.996257\tvalid_1's binary_logloss: 0.0856867\tvalid_1's ks: 0.957549\n",
      "[200]\ttraining's binary_logloss: 0.0360117\ttraining's ks: 0.997341\tvalid_1's binary_logloss: 0.083898\tvalid_1's ks: 0.957746\n",
      "100%|██████████| 1/1 [02:12<00:00, 132.65s/trial, best loss: -0.9549895599416933]\n",
      "Best parameters found: {'colsample_bytree': 0.819403055720703, 'learning_rate': 0.1736517195589613, 'max_depth': 4, 'min_child_weight': 0.9276203475646596, 'num_leaves': 111, 'reg_alpha': 0.6257533870573401, 'reg_lambda': 0.6797438602084419, 'scale_pos_weight': 7.043211714373893, 'subsample': 0.813207150345939}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:29:51.923280Z",
     "start_time": "2024-10-30T00:29:51.911327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5折交叉验证\n",
    "def lgb_5_fold(X, y, best_params, n_folds=5):\n",
    "    best_params.update({\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'verbose': -1,\n",
    "        'n_estimators': 10000,  # 和贝叶斯优化期间保持一致\n",
    "        'early_stopping_round': 20\n",
    "    })\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2023)\n",
    "    fold_scores = []\n",
    "    final_predictions = np.zeros(len(X))  # 用于存储每一折的预测结果\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        # 训练模型\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "        model = lgb.train(best_params,\n",
    "                          lgb_train,\n",
    "                          valid_sets=[lgb_train, lgb_valid],\n",
    "                          feval=lgb_ks_eval,\n",
    "                          callbacks=[lgb.early_stopping(stopping_rounds=50),\n",
    "                                     lgb.log_evaluation(10)])\n",
    "\n",
    "        # 保存模型\n",
    "        model_path = f\"{dir_model}/lgbm_fold_{fold + 1}.bin\"\n",
    "        model.save_model(model_path)\n",
    "        print(f\"Model for fold {fold + 1} saved at {model_path}\")\n",
    "\n",
    "        # 验证集预测\n",
    "        y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "        # 计算KS\n",
    "        score = ks_stat(y_valid, y_pred)\n",
    "        fold_scores.append(score)\n",
    "\n",
    "        # 将每一折的预测结果保存到对应的索引位置\n",
    "        final_predictions[valid_idx] = y_pred\n",
    "\n",
    "        print(f\"Fold {fold + 1} - Score: {score:.4f}\")\n",
    "\n",
    "    # 最终综合预测结果的评分\n",
    "    overall_score = ks_stat(y, final_predictions)\n",
    "\n",
    "    print(f\"\\nOverall Score: {overall_score:.4f}\")\n",
    "\n",
    "    return overall_score"
   ],
   "id": "967cc4696e9445aa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:31:33.953225Z",
     "start_time": "2024-10-30T00:29:51.925282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 执行5折交叉验证\n",
    "best_score = lgb_5_fold(X, y, best_params)\n",
    "print(f\"Best score: {best_score}\")\n"
   ],
   "id": "c7402558a4582707",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.499481\ttraining's ks: 0.82407\tvalid_1's binary_logloss: 0.50184\tvalid_1's ks: 0.818976\n",
      "[20]\ttraining's binary_logloss: 0.38321\ttraining's ks: 0.889419\tvalid_1's binary_logloss: 0.387246\tvalid_1's ks: 0.885849\n",
      "[30]\ttraining's binary_logloss: 0.299756\ttraining's ks: 0.915371\tvalid_1's binary_logloss: 0.304142\tvalid_1's ks: 0.911653\n",
      "[40]\ttraining's binary_logloss: 0.258261\ttraining's ks: 0.926009\tvalid_1's binary_logloss: 0.26323\tvalid_1's ks: 0.921208\n",
      "[50]\ttraining's binary_logloss: 0.229284\ttraining's ks: 0.932903\tvalid_1's binary_logloss: 0.23429\tvalid_1's ks: 0.927511\n",
      "[60]\ttraining's binary_logloss: 0.209526\ttraining's ks: 0.938295\tvalid_1's binary_logloss: 0.215004\tvalid_1's ks: 0.933518\n",
      "[70]\ttraining's binary_logloss: 0.194578\ttraining's ks: 0.940905\tvalid_1's binary_logloss: 0.200923\tvalid_1's ks: 0.936375\n",
      "[80]\ttraining's binary_logloss: 0.185424\ttraining's ks: 0.942727\tvalid_1's binary_logloss: 0.19239\tvalid_1's ks: 0.938049\n",
      "[90]\ttraining's binary_logloss: 0.174395\ttraining's ks: 0.945559\tvalid_1's binary_logloss: 0.182221\tvalid_1's ks: 0.940117\n",
      "[100]\ttraining's binary_logloss: 0.167684\ttraining's ks: 0.94716\tvalid_1's binary_logloss: 0.176412\tvalid_1's ks: 0.941693\n",
      "[110]\ttraining's binary_logloss: 0.160378\ttraining's ks: 0.948341\tvalid_1's binary_logloss: 0.169853\tvalid_1's ks: 0.943466\n",
      "[120]\ttraining's binary_logloss: 0.154363\ttraining's ks: 0.949745\tvalid_1's binary_logloss: 0.164551\tvalid_1's ks: 0.94386\n",
      "[130]\ttraining's binary_logloss: 0.149114\ttraining's ks: 0.950779\tvalid_1's binary_logloss: 0.160105\tvalid_1's ks: 0.945042\n",
      "[140]\ttraining's binary_logloss: 0.14435\ttraining's ks: 0.95201\tvalid_1's binary_logloss: 0.15592\tvalid_1's ks: 0.946125\n",
      "[150]\ttraining's binary_logloss: 0.140323\ttraining's ks: 0.953168\tvalid_1's binary_logloss: 0.153032\tvalid_1's ks: 0.946223\n",
      "[160]\ttraining's binary_logloss: 0.137638\ttraining's ks: 0.953611\tvalid_1's binary_logloss: 0.150722\tvalid_1's ks: 0.945928\n",
      "[170]\ttraining's binary_logloss: 0.133088\ttraining's ks: 0.954546\tvalid_1's binary_logloss: 0.147338\tvalid_1's ks: 0.946814\n",
      "[180]\ttraining's binary_logloss: 0.130008\ttraining's ks: 0.955507\tvalid_1's binary_logloss: 0.14503\tvalid_1's ks: 0.947307\n",
      "[190]\ttraining's binary_logloss: 0.126872\ttraining's ks: 0.956048\tvalid_1's binary_logloss: 0.142523\tvalid_1's ks: 0.947799\n",
      "[200]\ttraining's binary_logloss: 0.123268\ttraining's ks: 0.956368\tvalid_1's binary_logloss: 0.139956\tvalid_1's ks: 0.948193\n",
      "[210]\ttraining's binary_logloss: 0.120508\ttraining's ks: 0.957033\tvalid_1's binary_logloss: 0.137966\tvalid_1's ks: 0.948686\n",
      "[220]\ttraining's binary_logloss: 0.117449\ttraining's ks: 0.958018\tvalid_1's binary_logloss: 0.135743\tvalid_1's ks: 0.949277\n",
      "[230]\ttraining's binary_logloss: 0.114839\ttraining's ks: 0.959003\tvalid_1's binary_logloss: 0.13419\tvalid_1's ks: 0.948587\n",
      "[240]\ttraining's binary_logloss: 0.112537\ttraining's ks: 0.959545\tvalid_1's binary_logloss: 0.132566\tvalid_1's ks: 0.948292\n",
      "Model for fold 1 saved at /data/contest/2024\\2\\model/lgbm_fold_1.bin\n",
      "Fold 1 - Score: 0.9493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.508846\ttraining's ks: 0.826038\tvalid_1's binary_logloss: 0.511172\tvalid_1's ks: 0.821345\n",
      "[20]\ttraining's binary_logloss: 0.381318\ttraining's ks: 0.889861\tvalid_1's binary_logloss: 0.385355\tvalid_1's ks: 0.888416\n",
      "[30]\ttraining's binary_logloss: 0.306748\ttraining's ks: 0.912416\tvalid_1's binary_logloss: 0.31204\tvalid_1's ks: 0.911856\n",
      "[40]\ttraining's binary_logloss: 0.258188\ttraining's ks: 0.925244\tvalid_1's binary_logloss: 0.264391\tvalid_1's ks: 0.927023\n",
      "[50]\ttraining's binary_logloss: 0.231835\ttraining's ks: 0.931548\tvalid_1's binary_logloss: 0.239126\tvalid_1's ks: 0.92978\n",
      "[60]\ttraining's binary_logloss: 0.210633\ttraining's ks: 0.936325\tvalid_1's binary_logloss: 0.218348\tvalid_1's ks: 0.933227\n",
      "[70]\ttraining's binary_logloss: 0.197787\ttraining's ks: 0.940092\tvalid_1's binary_logloss: 0.205861\tvalid_1's ks: 0.936084\n",
      "[80]\ttraining's binary_logloss: 0.18597\ttraining's ks: 0.94312\tvalid_1's binary_logloss: 0.195079\tvalid_1's ks: 0.937659\n",
      "[90]\ttraining's binary_logloss: 0.176154\ttraining's ks: 0.945041\tvalid_1's binary_logloss: 0.185844\tvalid_1's ks: 0.939235\n",
      "[100]\ttraining's binary_logloss: 0.167465\ttraining's ks: 0.94738\tvalid_1's binary_logloss: 0.178317\tvalid_1's ks: 0.941205\n",
      "[110]\ttraining's binary_logloss: 0.16128\ttraining's ks: 0.948907\tvalid_1's binary_logloss: 0.172623\tvalid_1's ks: 0.943569\n",
      "[120]\ttraining's binary_logloss: 0.15683\ttraining's ks: 0.949498\tvalid_1's binary_logloss: 0.169208\tvalid_1's ks: 0.943766\n",
      "[130]\ttraining's binary_logloss: 0.150979\ttraining's ks: 0.950606\tvalid_1's binary_logloss: 0.163913\tvalid_1's ks: 0.945144\n",
      "[140]\ttraining's binary_logloss: 0.146513\ttraining's ks: 0.951615\tvalid_1's binary_logloss: 0.1601\tvalid_1's ks: 0.945637\n",
      "[150]\ttraining's binary_logloss: 0.141091\ttraining's ks: 0.952551\tvalid_1's binary_logloss: 0.155563\tvalid_1's ks: 0.945538\n",
      "[160]\ttraining's binary_logloss: 0.137775\ttraining's ks: 0.953191\tvalid_1's binary_logloss: 0.152805\tvalid_1's ks: 0.945834\n",
      "[170]\ttraining's binary_logloss: 0.133696\ttraining's ks: 0.954226\tvalid_1's binary_logloss: 0.149462\tvalid_1's ks: 0.946129\n",
      "[180]\ttraining's binary_logloss: 0.13026\ttraining's ks: 0.955137\tvalid_1's binary_logloss: 0.146693\tvalid_1's ks: 0.946523\n",
      "[190]\ttraining's binary_logloss: 0.127386\ttraining's ks: 0.955998\tvalid_1's binary_logloss: 0.144646\tvalid_1's ks: 0.94672\n",
      "[200]\ttraining's binary_logloss: 0.124086\ttraining's ks: 0.957131\tvalid_1's binary_logloss: 0.142276\tvalid_1's ks: 0.94672\n",
      "[210]\ttraining's binary_logloss: 0.121071\ttraining's ks: 0.957673\tvalid_1's binary_logloss: 0.14011\tvalid_1's ks: 0.947311\n",
      "[220]\ttraining's binary_logloss: 0.118618\ttraining's ks: 0.958633\tvalid_1's binary_logloss: 0.138442\tvalid_1's ks: 0.947902\n",
      "[230]\ttraining's binary_logloss: 0.116304\ttraining's ks: 0.959421\tvalid_1's binary_logloss: 0.13686\tvalid_1's ks: 0.947705\n",
      "[240]\ttraining's binary_logloss: 0.113611\ttraining's ks: 0.959864\tvalid_1's binary_logloss: 0.134983\tvalid_1's ks: 0.948197\n",
      "[250]\ttraining's binary_logloss: 0.111565\ttraining's ks: 0.960455\tvalid_1's binary_logloss: 0.133553\tvalid_1's ks: 0.948394\n",
      "[260]\ttraining's binary_logloss: 0.109375\ttraining's ks: 0.961243\tvalid_1's binary_logloss: 0.131936\tvalid_1's ks: 0.94869\n",
      "[270]\ttraining's binary_logloss: 0.107164\ttraining's ks: 0.961834\tvalid_1's binary_logloss: 0.130436\tvalid_1's ks: 0.948887\n",
      "[280]\ttraining's binary_logloss: 0.104617\ttraining's ks: 0.962745\tvalid_1's binary_logloss: 0.12865\tvalid_1's ks: 0.948887\n",
      "[290]\ttraining's binary_logloss: 0.102512\ttraining's ks: 0.963656\tvalid_1's binary_logloss: 0.127261\tvalid_1's ks: 0.949478\n",
      "[300]\ttraining's binary_logloss: 0.100026\ttraining's ks: 0.964567\tvalid_1's binary_logloss: 0.125634\tvalid_1's ks: 0.949576\n",
      "[310]\ttraining's binary_logloss: 0.0979801\ttraining's ks: 0.965281\tvalid_1's binary_logloss: 0.124593\tvalid_1's ks: 0.949281\n",
      "Model for fold 2 saved at /data/contest/2024\\2\\model/lgbm_fold_2.bin\n",
      "Fold 2 - Score: 0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.506099\ttraining's ks: 0.822495\tvalid_1's binary_logloss: 0.50771\tvalid_1's ks: 0.820447\n",
      "[20]\ttraining's binary_logloss: 0.392159\ttraining's ks: 0.883362\tvalid_1's binary_logloss: 0.394008\tvalid_1's ks: 0.880823\n",
      "[30]\ttraining's binary_logloss: 0.300865\ttraining's ks: 0.917342\tvalid_1's binary_logloss: 0.303704\tvalid_1's ks: 0.915197\n",
      "[40]\ttraining's binary_logloss: 0.256973\ttraining's ks: 0.927068\tvalid_1's binary_logloss: 0.260787\tvalid_1's ks: 0.923865\n",
      "[50]\ttraining's binary_logloss: 0.231194\ttraining's ks: 0.93246\tvalid_1's binary_logloss: 0.236092\tvalid_1's ks: 0.928494\n",
      "[60]\ttraining's binary_logloss: 0.211632\ttraining's ks: 0.937458\tvalid_1's binary_logloss: 0.21735\tvalid_1's ks: 0.934995\n",
      "[70]\ttraining's binary_logloss: 0.196117\ttraining's ks: 0.940635\tvalid_1's binary_logloss: 0.202792\tvalid_1's ks: 0.937654\n",
      "[80]\ttraining's binary_logloss: 0.186067\ttraining's ks: 0.942605\tvalid_1's binary_logloss: 0.193561\tvalid_1's ks: 0.938934\n",
      "[90]\ttraining's binary_logloss: 0.176649\ttraining's ks: 0.944722\tvalid_1's binary_logloss: 0.185261\tvalid_1's ks: 0.941101\n",
      "[100]\ttraining's binary_logloss: 0.169259\ttraining's ks: 0.946175\tvalid_1's binary_logloss: 0.178709\tvalid_1's ks: 0.942185\n",
      "[110]\ttraining's binary_logloss: 0.162513\ttraining's ks: 0.947923\tvalid_1's binary_logloss: 0.173266\tvalid_1's ks: 0.94317\n",
      "[120]\ttraining's binary_logloss: 0.157053\ttraining's ks: 0.948613\tvalid_1's binary_logloss: 0.168275\tvalid_1's ks: 0.943957\n",
      "[130]\ttraining's binary_logloss: 0.151525\ttraining's ks: 0.94945\tvalid_1's binary_logloss: 0.163918\tvalid_1's ks: 0.944745\n",
      "[140]\ttraining's binary_logloss: 0.146304\ttraining's ks: 0.950681\tvalid_1's binary_logloss: 0.159884\tvalid_1's ks: 0.945336\n",
      "[150]\ttraining's binary_logloss: 0.142294\ttraining's ks: 0.951641\tvalid_1's binary_logloss: 0.156771\tvalid_1's ks: 0.946321\n",
      "[160]\ttraining's binary_logloss: 0.137771\ttraining's ks: 0.952651\tvalid_1's binary_logloss: 0.153124\tvalid_1's ks: 0.946715\n",
      "[170]\ttraining's binary_logloss: 0.134111\ttraining's ks: 0.953463\tvalid_1's binary_logloss: 0.150526\tvalid_1's ks: 0.947011\n",
      "[180]\ttraining's binary_logloss: 0.130621\ttraining's ks: 0.954226\tvalid_1's binary_logloss: 0.147749\tvalid_1's ks: 0.947602\n",
      "[190]\ttraining's binary_logloss: 0.127516\ttraining's ks: 0.954694\tvalid_1's binary_logloss: 0.145572\tvalid_1's ks: 0.947799\n",
      "[200]\ttraining's binary_logloss: 0.12491\ttraining's ks: 0.955261\tvalid_1's binary_logloss: 0.143791\tvalid_1's ks: 0.948094\n",
      "[210]\ttraining's binary_logloss: 0.122167\ttraining's ks: 0.955999\tvalid_1's binary_logloss: 0.141919\tvalid_1's ks: 0.948882\n",
      "[220]\ttraining's binary_logloss: 0.120109\ttraining's ks: 0.956492\tvalid_1's binary_logloss: 0.140255\tvalid_1's ks: 0.948882\n",
      "[230]\ttraining's binary_logloss: 0.116876\ttraining's ks: 0.957526\tvalid_1's binary_logloss: 0.137922\tvalid_1's ks: 0.949178\n",
      "[240]\ttraining's binary_logloss: 0.114266\ttraining's ks: 0.958388\tvalid_1's binary_logloss: 0.136089\tvalid_1's ks: 0.949473\n",
      "[250]\ttraining's binary_logloss: 0.111622\ttraining's ks: 0.959053\tvalid_1's binary_logloss: 0.134309\tvalid_1's ks: 0.949276\n",
      "[260]\ttraining's binary_logloss: 0.109541\ttraining's ks: 0.95984\tvalid_1's binary_logloss: 0.132749\tvalid_1's ks: 0.949572\n",
      "[270]\ttraining's binary_logloss: 0.106931\ttraining's ks: 0.960727\tvalid_1's binary_logloss: 0.130807\tvalid_1's ks: 0.949769\n",
      "[280]\ttraining's binary_logloss: 0.104172\ttraining's ks: 0.961933\tvalid_1's binary_logloss: 0.128991\tvalid_1's ks: 0.949572\n",
      "[290]\ttraining's binary_logloss: 0.101786\ttraining's ks: 0.962721\tvalid_1's binary_logloss: 0.127318\tvalid_1's ks: 0.949966\n",
      "Model for fold 3 saved at /data/contest/2024\\2\\model/lgbm_fold_3.bin\n",
      "Fold 3 - Score: 0.9503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.51032\ttraining's ks: 0.817497\tvalid_1's binary_logloss: 0.508416\tvalid_1's ks: 0.822122\n",
      "[20]\ttraining's binary_logloss: 0.381092\ttraining's ks: 0.891636\tvalid_1's binary_logloss: 0.379287\tvalid_1's ks: 0.896188\n",
      "[30]\ttraining's binary_logloss: 0.307252\ttraining's ks: 0.914215\tvalid_1's binary_logloss: 0.305225\tvalid_1's ks: 0.917758\n",
      "[40]\ttraining's binary_logloss: 0.263285\ttraining's ks: 0.925147\tvalid_1's binary_logloss: 0.262146\tvalid_1's ks: 0.9281\n",
      "[50]\ttraining's binary_logloss: 0.236256\ttraining's ks: 0.931992\tvalid_1's binary_logloss: 0.23616\tvalid_1's ks: 0.933616\n",
      "[60]\ttraining's binary_logloss: 0.212382\ttraining's ks: 0.93704\tvalid_1's binary_logloss: 0.213158\tvalid_1's ks: 0.939427\n",
      "[70]\ttraining's binary_logloss: 0.197519\ttraining's ks: 0.940684\tvalid_1's binary_logloss: 0.199441\tvalid_1's ks: 0.941101\n",
      "[80]\ttraining's binary_logloss: 0.185561\ttraining's ks: 0.943393\tvalid_1's binary_logloss: 0.188158\tvalid_1's ks: 0.943366\n",
      "[90]\ttraining's binary_logloss: 0.176373\ttraining's ks: 0.945091\tvalid_1's binary_logloss: 0.179952\tvalid_1's ks: 0.945435\n",
      "[100]\ttraining's binary_logloss: 0.170398\ttraining's ks: 0.946692\tvalid_1's binary_logloss: 0.17459\tvalid_1's ks: 0.946223\n",
      "[110]\ttraining's binary_logloss: 0.163973\ttraining's ks: 0.947751\tvalid_1's binary_logloss: 0.169136\tvalid_1's ks: 0.946715\n",
      "[120]\ttraining's binary_logloss: 0.15856\ttraining's ks: 0.949327\tvalid_1's binary_logloss: 0.16443\tvalid_1's ks: 0.947306\n",
      "[130]\ttraining's binary_logloss: 0.153873\ttraining's ks: 0.950016\tvalid_1's binary_logloss: 0.16053\tvalid_1's ks: 0.947996\n",
      "[140]\ttraining's binary_logloss: 0.149007\ttraining's ks: 0.951001\tvalid_1's binary_logloss: 0.156534\tvalid_1's ks: 0.94967\n",
      "[150]\ttraining's binary_logloss: 0.14427\ttraining's ks: 0.95201\tvalid_1's binary_logloss: 0.153085\tvalid_1's ks: 0.949769\n",
      "[160]\ttraining's binary_logloss: 0.140574\ttraining's ks: 0.952897\tvalid_1's binary_logloss: 0.150349\tvalid_1's ks: 0.950261\n",
      "[170]\ttraining's binary_logloss: 0.137381\ttraining's ks: 0.953414\tvalid_1's binary_logloss: 0.148119\tvalid_1's ks: 0.950556\n",
      "[180]\ttraining's binary_logloss: 0.13395\ttraining's ks: 0.954374\tvalid_1's binary_logloss: 0.145428\tvalid_1's ks: 0.95095\n",
      "[190]\ttraining's binary_logloss: 0.130529\ttraining's ks: 0.955039\tvalid_1's binary_logloss: 0.142753\tvalid_1's ks: 0.951344\n",
      "[200]\ttraining's binary_logloss: 0.127667\ttraining's ks: 0.955507\tvalid_1's binary_logloss: 0.140526\tvalid_1's ks: 0.951443\n",
      "[210]\ttraining's binary_logloss: 0.124197\ttraining's ks: 0.956221\tvalid_1's binary_logloss: 0.138293\tvalid_1's ks: 0.951738\n",
      "[220]\ttraining's binary_logloss: 0.120609\ttraining's ks: 0.957206\tvalid_1's binary_logloss: 0.135629\tvalid_1's ks: 0.952625\n",
      "[230]\ttraining's binary_logloss: 0.118397\ttraining's ks: 0.957846\tvalid_1's binary_logloss: 0.134361\tvalid_1's ks: 0.952034\n",
      "[240]\ttraining's binary_logloss: 0.115118\ttraining's ks: 0.958683\tvalid_1's binary_logloss: 0.131974\tvalid_1's ks: 0.952329\n",
      "Model for fold 4 saved at /data/contest/2024\\2\\model/lgbm_fold_4.bin\n",
      "Fold 4 - Score: 0.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\ttraining's binary_logloss: 0.509617\ttraining's ks: 0.819787\tvalid_1's binary_logloss: 0.510997\tvalid_1's ks: 0.815621\n",
      "[20]\ttraining's binary_logloss: 0.383188\ttraining's ks: 0.886342\tvalid_1's binary_logloss: 0.386306\tvalid_1's ks: 0.883286\n",
      "[30]\ttraining's binary_logloss: 0.305895\ttraining's ks: 0.911531\tvalid_1's binary_logloss: 0.30945\tvalid_1's ks: 0.908697\n",
      "[40]\ttraining's binary_logloss: 0.26102\ttraining's ks: 0.924975\tvalid_1's binary_logloss: 0.265537\tvalid_1's ks: 0.921206\n",
      "[50]\ttraining's binary_logloss: 0.232969\ttraining's ks: 0.932017\tvalid_1's binary_logloss: 0.237975\tvalid_1's ks: 0.928199\n",
      "[60]\ttraining's binary_logloss: 0.214334\ttraining's ks: 0.936178\tvalid_1's binary_logloss: 0.220197\tvalid_1's ks: 0.933813\n",
      "[70]\ttraining's binary_logloss: 0.198946\ttraining's ks: 0.939034\tvalid_1's binary_logloss: 0.205345\tvalid_1's ks: 0.937063\n",
      "[80]\ttraining's binary_logloss: 0.184281\ttraining's ks: 0.942974\tvalid_1's binary_logloss: 0.191576\tvalid_1's ks: 0.9412\n",
      "[90]\ttraining's binary_logloss: 0.175427\ttraining's ks: 0.944525\tvalid_1's binary_logloss: 0.183743\tvalid_1's ks: 0.942283\n",
      "[100]\ttraining's binary_logloss: 0.169575\ttraining's ks: 0.946175\tvalid_1's binary_logloss: 0.178848\tvalid_1's ks: 0.94376\n",
      "[110]\ttraining's binary_logloss: 0.162894\ttraining's ks: 0.947652\tvalid_1's binary_logloss: 0.173112\tvalid_1's ks: 0.945336\n",
      "[120]\ttraining's binary_logloss: 0.15692\ttraining's ks: 0.948809\tvalid_1's binary_logloss: 0.168329\tvalid_1's ks: 0.946715\n",
      "[130]\ttraining's binary_logloss: 0.152038\ttraining's ks: 0.949573\tvalid_1's binary_logloss: 0.164359\tvalid_1's ks: 0.947799\n",
      "[140]\ttraining's binary_logloss: 0.147849\ttraining's ks: 0.950041\tvalid_1's binary_logloss: 0.161134\tvalid_1's ks: 0.948587\n",
      "[150]\ttraining's binary_logloss: 0.144605\ttraining's ks: 0.950681\tvalid_1's binary_logloss: 0.158529\tvalid_1's ks: 0.949178\n",
      "[160]\ttraining's binary_logloss: 0.139968\ttraining's ks: 0.951764\tvalid_1's binary_logloss: 0.15473\tvalid_1's ks: 0.950359\n",
      "[170]\ttraining's binary_logloss: 0.136647\ttraining's ks: 0.952872\tvalid_1's binary_logloss: 0.152194\tvalid_1's ks: 0.95095\n",
      "[180]\ttraining's binary_logloss: 0.132678\ttraining's ks: 0.953931\tvalid_1's binary_logloss: 0.149273\tvalid_1's ks: 0.952034\n",
      "[190]\ttraining's binary_logloss: 0.129479\ttraining's ks: 0.954202\tvalid_1's binary_logloss: 0.147113\tvalid_1's ks: 0.951837\n",
      "[200]\ttraining's binary_logloss: 0.126157\ttraining's ks: 0.955384\tvalid_1's binary_logloss: 0.144976\tvalid_1's ks: 0.951541\n",
      "Model for fold 5 saved at /data/contest/2024\\2\\model/lgbm_fold_5.bin\n",
      "Fold 5 - Score: 0.9522\n",
      "\n",
      "Overall Score: 0.9499\n",
      "Best score: 0.9499074183508648\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_seed(X, y, param_space, n_folds=5, seed_list=[0, 42, 100, 2023]):\n",
    "    best_seed = None\n",
    "    lowest_std = float('inf')\n",
    "    seed_scores = {}\n",
    "\n",
    "    for seed in seed_list:\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        fold_scores = []\n",
    "\n",
    "        for train_idx, valid_idx in skf.split(X, y):\n",
    "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "            y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "            # 使用与之前一致的参数和模型训练方法\n",
    "            model = lgb.LGBMClassifier(**param_space)  # 使用参数构建模型\n",
    "            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=False)\n",
    "            \n",
    "            y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "            ks = ks_stat(y_valid, y_pred)\n",
    "            fold_scores.append(ks)\n",
    "\n",
    "        # 计算该种子下的折间KS值的标准差\n",
    "        std_ks = np.std(fold_scores)\n",
    "        seed_scores[seed] = (np.mean(fold_scores), std_ks)\n",
    "\n",
    "        if std_ks < lowest_std:\n",
    "            best_seed = seed\n",
    "            lowest_std = std_ks\n",
    "\n",
    "    print(f\"Best seed found: {best_seed} with lowest KS std: {lowest_std}\")\n",
    "    return best_seed, seed_scores\n",
    "\n",
    "\n",
    "optimal_seed, seed_scores = find_optimal_seed(X, y, best_params, n_folds=5, seed_list=[0, 42, 100, 2023])\n",
    "\n",
    "print(optimal_seed)\n",
    "print(seed_scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T00:08:18.115262Z",
     "start_time": "2024-10-30T00:08:18.085078Z"
    }
   },
   "id": "40b4c8da6ccf2a1c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 37\u001B[0m\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest seed found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_seed\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with lowest KS std: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlowest_std\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_seed, seed_scores\n\u001B[1;32m---> 37\u001B[0m optimal_seed, seed_scores \u001B[38;5;241m=\u001B[39m find_optimal_seed(X, y, \u001B[43mbest_params\u001B[49m, n_folds\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, seed_list\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m42\u001B[39m, \u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m2023\u001B[39m])\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(optimal_seed)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(seed_scores)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'best_params' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# 测试集预测并生成结果文件\n",
    "def predict_test_set(test_data, n_folds=5, output_file='upload.csv'):\n",
    "    test_predictions = np.zeros(len(test_data))\n",
    "\n",
    "    # 依次加载每一折的模型进行预测\n",
    "    for fold in range(n_folds):\n",
    "        model_path = f\"{dir_model}/lgbm_fold_{fold + 1}.bin\"\n",
    "        model = lgb.Booster(model_file=model_path)\n",
    "\n",
    "        # 对测试集进行预测，并将每一折的预测加总\n",
    "        test_predictions += model.predict(test_data)\n",
    "\n",
    "    # 平均每一折的预测\n",
    "    test_predictions /= n_folds\n",
    "\n",
    "    # 生成预测文件\n",
    "    submission = pd.DataFrame({'CUST_NO': test_data.index, 'PRED': test_predictions})\n",
    "    submission.to_csv(output_file, index=False)\n",
    "    print(f\"Test predictions saved to {output_file}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T00:32:48.562235Z",
     "start_time": "2024-10-30T00:32:48.546608Z"
    }
   },
   "id": "fa86b3eaf4ad5a9d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "X_test = pd.read_csv(f'{dir_preprocess}/test.csv')\n",
    "X_test.set_index('CUST_NO', inplace=True)\n",
    "predict_test_set(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-30T00:36:04.863379Z",
     "start_time": "2024-10-30T00:36:03.390403Z"
    }
   },
   "id": "9e91bbb44f99635f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to upload.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T13:26:18.150949Z",
     "start_time": "2024-10-29T13:26:17.666298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = f\"{dir_model}/lgbm_fold_{0 + 1}.bin\"\n",
    "model = lgb.Booster(model_file=model_path)\n",
    "feature_importance = model.feature_importance(importance_type='gain')\n",
    "feature_names = X.fea\n",
    "# 构建重要性数据框\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importance})\n",
    "\n",
    "# 筛选重要性较低的特征\n",
    "low_importance_features = importance_df[importance_df['importance'] < threshold]['feature']"
   ],
   "id": "19df2b12c848b8cc",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m feature_names \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# 构建重要性数据框\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m importance_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfeature\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimportance\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_importance\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# 筛选重要性较低的特征\u001B[39;00m\n\u001B[0;32m      9\u001B[0m low_importance_features \u001B[38;5;241m=\u001B[39m importance_df[importance_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m<\u001B[39m threshold][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mD:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\pandas\\core\\frame.py:664\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    658\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[0;32m    659\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[0;32m    660\u001B[0m     )\n\u001B[0;32m    662\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    663\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[1;32m--> 664\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    665\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[0;32m    666\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmrecords\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmrecords\u001B[39;00m\n",
      "File \u001B[1;32mD:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[1;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    490\u001B[0m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[0;32m    491\u001B[0m         arrays \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[1;32m--> 493\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_integrity:\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;66;03m# figure out the index, if necessary\u001B[39;00m\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 118\u001B[0m         index \u001B[38;5;241m=\u001B[39m \u001B[43m_extract_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m         index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n",
      "File \u001B[1;32mD:\\workspace\\misc\\OneDay\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001B[0m, in \u001B[0;36m_extract_index\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    664\u001B[0m lengths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(raw_lengths))\n\u001B[0;32m    665\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(lengths) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 666\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll arrays must be of the same length\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    668\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m have_dicts:\n\u001B[0;32m    669\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    670\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    671\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: All arrays must be of the same length"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "42941f9bcd0ed791"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
